{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to oneAPI and DPC++\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Explain how __oneAPI__ can solve the challenges of programming in a heterogeneous world \n",
    "* Use oneAPI solutions to enable your workflows\n",
    "* Understand the __Data Parallel C++ (DPC++)__ language and programming model\n",
    "* Familiarization on the use Jupyter notebooks for training throughout the course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oneAPI Software Model Overview\n",
    "The __oneAPI__ software model provides a comprehensive and unified portfolio of developer tools that can\n",
    "be used across hardware targets, including a range of performance libraries spanning several workload\n",
    "domains. The libraries include functions custom-coded for each target architecture so the same\n",
    "function call delivers optimized performance across supported architectures. __DPC++__ is based on\n",
    "industry standards and open specifications to encourage ecosystem collaboration and innovation.\n",
    "\n",
    "### oneAPI Distribution\n",
    "oneAPI is available via multiple distribution channels:\n",
    "* Local product installation: install the oneAPI toolkits from the __Intel® Developer Zone__.\n",
    "* Install from containers or repositories: install the oneAPI toolkits from one of several supported\n",
    "containers or repositories.\n",
    "* Pre-installed in the __Intel® DevCloud__: use a free development sandbox for access to the latest Intel® SVMS hardware and select oneAPI software. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Challenges for Multiple architectures\n",
    "Currently in the data centric space there is growth in specialized workloads. Each kind of data centric hardware typically needs to be programmed using different languages and libraries as there is no common programming language or APIs, this requires maintainig seperate code bases. Developers have to learn a whole set of different tools as there is inconsistent tool support across platforms. Developing software for each hardware platform requires a separate investment, with little ability to reuse that work to target a different architecture. You will also have to consider the requirement of the diverse set of data-centric hardware.\n",
    "\n",
    "<img src=\"Assets/oneapi1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing oneAPI\n",
    "__oneAPI__ is a solution to deliver unified programming model to __simplify development__ across diverse architectures. It includes a unified and simplified language and libraries for expressing __parallelism__ and delivers uncompromised native high-level language performance across a range of hardware including __CPUs, GPUs, FPGAs__. oneAPI initiative is based on __industry standards and open specifications__ and is interoperable with existing HPC programming models\n",
    "\n",
    "<img src=\"Assets/oneapi2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Simple Exercise\n",
    "The exercise introduces DPC++ to the developer by way of a small simple code. In addition, it introduces the developer to the Jupyter notebook environment for editing and saving code; and for running and submitting programs to the Intel® DevCloud.\n",
    "\n",
    "##  Editing the simple.cpp code\n",
    "The Jupyter cell below with the gray background can be edited in-place and saved.\n",
    "\n",
    "The first line of the cell contains the command **%%writefile 'simple.cpp'** This tells the input cell to save the contents of the cell into the file name 'simple.cpp'  As you edit the cell and run it, it will save your changes into that file.\n",
    "\n",
    "The code below shows simple DPC++ code intended for you to just get started to the devcloud environment. Inspect code, there are no modifications necessary. \n",
    "1. Inspect the code cell below and click run ▶ to save the code to file\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/simple.cpp\n",
    "//==============================================================\n",
    "// Copyright © 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <CL/sycl.hpp>\n",
    "using namespace sycl;\n",
    "static const int N = 16;\n",
    "int main(){\n",
    "    //# define queue which has default device associated for offload\n",
    "    queue q;\n",
    "    std::cout << \"Device: \" << q.get_device().get_info<info::device::name>() << std::endl;\n",
    "\n",
    "    //# Unified Shared Memory Allocation enables data access on host and device\n",
    "    int *data = static_cast<int*>(malloc_shared(N * sizeof(int), q));\n",
    "\n",
    "    //# Initialization\n",
    "    for(int i=0; i<N; i++) data[i] = i;\n",
    "\n",
    "    //# Offload parallel computation to device\n",
    "    q.parallel_for(range<1>(N), [=] (id<1> i){\n",
    "        data[i] *= 2;\n",
    "    }).wait();\n",
    "\n",
    "    //# Print Output\n",
    "    for(int i=0; i<N; i++) std::cout << data[i] << std::endl;\n",
    "\n",
    "    free(data, q);\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_simple.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_simple.sh; else ./run_simple.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SYCL\n",
    "__SYCL__ (pronounced ‘sickle’) represents an industry standardization effort that includes\n",
    "support for data-parallel programming for C++. It is summarized as “C++ Single-source\n",
    "Heterogeneous Programming for OpenCL.” The SYCL standard, like OpenCL, is managed\n",
    "by the __Khronos Group*__.\n",
    "\n",
    "SYCL is a cross-platform abstraction layer that builds on OpenCL. It enables code\n",
    "for heterogeneous processors to be written in a “single source” style using C++. This is not\n",
    "only useful to the programmers, but it also gives a compiler the ability to analyze and\n",
    "optimize across the entire program regardless of the device on which the code is to be run.\n",
    "\n",
    "Unlike OpenCL, SYCL includes templates and lambda functions to enable higherlevel application software to be cleanly coded with optimized acceleration of kernel code.\n",
    "Developers program at a higher level than OpenCL but always have access to lower-level code through seamless integration with OpenCL, as well as C/C++ libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Parallel C++\n",
    "__oneAPI__ programs are written in __Data Parallel C++ (DPC++)__. It is based on modern C++ productivity benefits and familiar constructs and incorporates the __SYCL*__ standard for data parallelism and heterogeneous programming. DPC++ is a __single source__ where host code and __heterogeneous accelerator kernels__ can be mixed in same source files. A DPC++ program is invoked on the host computer and offloads the computation to an accelerator. Programmers use familiar C++ and library constructs with added functionliaties like a __queue__ for work targeting, __buffer__ for data management, and __parallel_for__ for parallelism to direct which parts of the computation and data should be offloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPC++ extends SYCL 1.2.1\n",
    "DPC++ programs __enhance productivity__ where simple things should be simple to express and reduce verbosity and programmer burden. They also __enhance performance__ by giving programmers control over program execution and by enabling hardware-specific features. It is fast-moving open collaboration feeding into the __SYCL* standard__ and is __open source__ implementation with goal of upstream LLVM and DPC++ extensions aim to become core __SYCL*__, or __Khronos*__ extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC oneAPI Single Node Workflow \n",
    "Accelerated code can be written in either a kernel (DPC++) or __directive based style__. Developers can use the __Intel® DPC++ Compatibility tool__ to perform a one-time migration from __CUDA__ to __Data Parallel C++__. Existing __Fortran__ applications can use a __directive style based on OpenMP__. Existing __C++__ applications can choose either the __Kernel style__ or the __directive based style option__ and existing __OpenCL__ applications can remain in the OpenCL language or migrate to Data Parallel C++.\n",
    "\n",
    "__Intel® Advisor__ is recommended step to  __Optimize__ the design for __vectorization and memory__ (CPU and GPU) and __Identify__ loops that are candidates for __offload__ and project the __performance on target accelerators.__\n",
    "\n",
    "The below is for reference to show the recommended approach for different starting points for HPC developers.\n",
    "\n",
    "\n",
    "<img src=\"Assets/workflow.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Compile & Run DPC++ program?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Simple Steps\n",
    "1. Initialize environment variables\n",
    "2. Compile the DPC++ source code\n",
    "3. Run the application\n",
    " \n",
    "#### Compiling and Running on DevCloud:\n",
    " \n",
    "For this training purposes we have written a script (q) to ease developers to run on DevCloud, which does the job of submiting the `run.sh` script to a gpu node on DevCloud for execution, waits for the job to complete and prints out the output/errors. We will be using this command to run on DevCloud: `./q run.sh`\n",
    "\n",
    "\n",
    "\n",
    "#### Compiling and Running on local system:\n",
    "\n",
    "If you have installed oneAPI Basekit on your local system, then you can use the commands below to compile and run a DPC++ program:\n",
    "\n",
    "    source /opt/intel/inteloneapi/setvars.sh\n",
    "\n",
    "    dpcpp simple.cpp -o simple\n",
    "\n",
    "    ./simple\n",
    "    \n",
    "_Note: run.sh script is a combination of the above three steps._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Exercise: Simple Vector Increment TO Vector Add\n",
    "### Code Walkthrough\n",
    "\n",
    "__DPC++ programs__ are standard C++. The program is invoked on the __host__ computer, and offloads computation to the __accelerator__. A programmer uses DPC++’s __queue, buffer, device, and kernel abstractions__ to direct which parts of the computation and data should be offloaded.\n",
    "\n",
    "The DPC++ compiler and the oneAPI libraries automates the tedious and error-prone aspects of compute and data offload, but still allow the programmer to control how computation and data are distributed for best performance. The compiler knows how to generate code for both the host and the accelerator, how to launch computation on the accelerator, and how to move data back and forth. \n",
    "\n",
    "In the below program we write a data parallel algorithm with DPC++ to leverage the computational power in __heterogenous computers__. The DPC++ platform model includes a host computer and a device. The host offloads computation to the device, which could be a __GPU, FPGA, or a multi-core CPU__.\n",
    "\n",
    "As a first step in a DPC++ program we create a __queue__. We offload computation to a __device__ by submitting tasks to a queue. The programmer can choose CPU, GPU, FPGA, and other devices through the __selector__. This program uses the default  q here, which means DPC++ runtime selects the most capable device available at runtime by using the default selector. We will talk about the devices, device selectors, and the concepts of buffers, accessors and kernels in the upcoming modules but below is a simple DPC++ program for you to get started with the above concepts.\n",
    "\n",
    "Device and host can either share physical __memory__ or have distinct memories. When the memories are distinct, offloading computation requires __copying data between host and device__. DPC++ does not require the programmer to manage the data copies. By creating __Buffers and Accessors__, DPC++ ensures that the data is available to host and device without any programmer effort. DPC++ also allows the programmer explicit control over data movement when it is necessary to achieve best peformance.\n",
    "\n",
    "In a DPC++ program, we define a __kernel__, which is applied to every point in an index space. For simple programs like this one, the index space maps directly to the elements of the array. The kernel is encapsulated in a __C++ lambda function__. The lambda function is passed a point in the index space as an array of coordinates. For this simple program, the index space coordinate is the same as the array index. The __parallel_for__ in the below program applies the lambda to the index space. The index space is defined in the first argument of the parallel_for as a 1 dimensional __range from 0 to N-1__.\n",
    "\n",
    "The __parallel_for__ is nested inside another lamba function, which is passed as an argument in the below program where we __submit to the queue__. The DPC++ runtime invokes the lambda when the accelerator connected to the queue is ready. The handler argument to the lambda allows operations inside the lambda to define the __data and dependences__ with other computation that may be executed on host or devices. We  will see more of this in later modules.\n",
    "\n",
    "Finally, the program does a __q.wait()__ on the queue. The earlier submit operation queues up an operation to be performed at a later time and immmediately returns. If the host wants to see the result of the computation, it must wait for the work to complete with a wait. Sometimes the device will encounter an error. The q.wait_and_throw() is a way for the host to capture and handle the error that has happened on the device.\n",
    "\n",
    "### Lab Exercise\n",
    "Vector increment is the “hello world” of data parallel computing. A vector is an array of data elements, and the below program performs the same computation on each element of the vector by adding 1. The code below shows Simple Vector Increment DPC++ code. We will change the program to create a new vector and add the elements in the new vector to the existing vector using DPC++.\n",
    "\n",
    "1. Select the code cell below, __follow the STEPS 1 to 6__ in the code comments to change from vector-increment to vector-add and click run ▶ to save the code to file\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/simple-vector-incr.cpp\n",
    "//==============================================================\n",
    "// Copyright © 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <CL/sycl.hpp>\n",
    "using namespace sycl;\n",
    "//N is set as 2 as this is just for demonstration purposes. Even if you make N bigger than 2 the program still\n",
    "//counts N as only 2 as the first 2 elements are only initialized here and the rest all becomes zero.\n",
    "static const size_t N = 2;\n",
    "\n",
    "// ############################################################\n",
    "// work\n",
    "\n",
    "void work(queue &q) {\n",
    "  std::cout << \"Device : \"\n",
    "            << q.get_device().get_info<info::device::name>()\n",
    "            << std::endl;\n",
    "  // ### Step 1 - Inspect\n",
    "  // The code presents one input buffer (vector1) for which Sycl buffer memory\n",
    "  // is allocated. The associated with vector1_accessor set to read/write gets\n",
    "  // the contents of the buffer.\n",
    "  int vector1[N] = {10, 10};\n",
    "  std::cout << \"Input  : \" << vector1[0] << \", \" << vector1[1] << std::endl;\n",
    "\n",
    "  // ### Step 2 - Add another input vector - vector2\n",
    "  // Uncomment the following line to add input vector2\n",
    "  //int vector2[N] = {20,20};\n",
    "\n",
    "  // ### Step 3 - Print out for vector2\n",
    "  // Uncomment the following line\n",
    "  //std::cout << \"Input  : \" << vector2[0] << \", \" << vector2[1] << std::endl;\n",
    "  buffer<int, 1> vector1_buffer(vector1, range<1>(N));\n",
    "\n",
    "  // ### Step 4 - Add another Sycl buffer - vector2_buffer\n",
    "  // Uncomment the following line\n",
    "  //buffer<int, 1> vector2_buffer(vector2, range<1>(N));\n",
    "  q.submit([&](handler &h) {\n",
    "    auto vector1_accessor =\n",
    "        vector1_buffer.get_access<access::mode::read_write>(h);\n",
    "\n",
    "    // Step 5 - add an accessor for vector2_buffer\n",
    "    // Uncomment the following line to add an accessor for vector 2\n",
    "    //auto vector2_accessor = vector2_buffer.get_access <access::mode::read >(h);\n",
    "\n",
    "    h.parallel_for<class test>(range<1>(N), [=](id<1> index) {\n",
    "      // ### Step 6 - Replace the existing vector1_accessor to accumulate\n",
    "      // vector2_accessor \n",
    "      // Comment the following line\n",
    "      vector1_accessor[index] += 1;\n",
    "\n",
    "      // Uncomment the following line\n",
    "      //vector1_accessor[index] += vector2_accessor[index];\n",
    "    });\n",
    "  });\n",
    "  q.wait();\n",
    "  vector1_buffer.get_access<access::mode::read>();\n",
    "  std::cout << \"Output : \" << vector1[0] << \", \" << vector1[1] << std::endl;\n",
    "}\n",
    "\n",
    "// ############################################################\n",
    "// entry point for the program\n",
    "\n",
    "int main() {  \n",
    "  try {\n",
    "    queue q;\n",
    "    work(q);\n",
    "  } catch (exception e) {\n",
    "    std::cerr << \"Exception: \" << e.what() << std::endl;\n",
    "    std::terminate();\n",
    "  } catch (...) {\n",
    "    std::cerr << \"Unknown exception\" << std::endl;\n",
    "    std::terminate();\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_simple-vector-incr.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_simple-vector-incr.sh; else ./run_simple-vector-incr.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "- [simple-vector-add.cpp](src/simple-vector-add.cpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this module you will have learned the following:\n",
    "* How oneAPI solves the challenges of programming in a heterogeneous world \n",
    "* Take advantage of oneAPI solutions to enable your workflows\n",
    "* Use the Intel® DevCloud to test-drive oneAPI tools and libraries\n",
    "* Introduced to DPC++ language and programming model\n",
    "* Become familiarized with the use of Juypter notebooks by editing of source code in context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><body><span style=\"color:green\"><h1>Survey</h1></span></body></html>\n",
    "\n",
    "[We would appreciate any feedback you’d care to give, so that we can improve the overall training quality and experience. Thanks! ](https://intel.az1.qualtrics.com/jfe/form/SV_6m4G7BXPNSS7FBz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Check out available resources\n",
    "\n",
    "#### Intel® oneAPI\n",
    "* [oneAPI main page](https://software.intel.com/oneapi \"oneAPI main page\")\n",
    "* [oneAPI programming guide](https://software.intel.com/sites/default/files/oneAPIProgrammingGuide_3.pdf \"oneAPI programming guide\")\n",
    "* [Intel® DevCloud Signup](https://software.intel.com/en-us/devcloud/oneapi \"Intel DevCloud\")  Sign up here if you do not have an account.\n",
    "* [Intel® DevCloud Connect](https://devcloud.intel.com/datacenter/connect)  Connect to the DevCloud here.\n",
    "* [Get Started with oneAPI for Linux*](https://software.intel.com/en-us/get-started-with-intel-oneapi-linux)\n",
    "* [Get Started with oneAPI for Windows*](https://software.intel.com/en-us/get-started-with-intel-oneapi-windows)\n",
    "* [oneAPI Sample Codes](https://software.intel.com/en-us/articles/code-samples-for-intel-oneapibeta-toolkits)\n",
    "* [oneAPI Specification elements](https://www.oneapi.com/spec/)\n",
    "\n",
    "#### SYCL \n",
    "* [SYCL* Specification (for version 1.2.1)](https://www.khronos.org/registry/SYCL/specs/sycl-1.2.1.pdf)\n",
    "\n",
    "#### Modern C++\n",
    "* [CPPReference](https://en.cppreference.com/w/)\n",
    "* [CPlusPlus](http://www.cplusplus.com/)\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
